# Government Procurement Automation System

A unified automation system for monitoring and processing government procurement job postings from multiple portals (HHSC, VMS, Odoo) with integrated RAG-powered chatbot capabilities.

## ğŸš€ Features

### Core Automation
- **Multi-Portal Monitoring**: Automated monitoring of HHSC and VMS portals for new job postings
- **Email Processing**: Automatic Gmail integration for processing job-related emails
- **Dual Table Processing**: Advanced job tracking using Excel-based dual table system
- **Continuous Monitoring**: Background monitoring with 15-second intervals for new emails
- **Daily Email Reports**: Automated daily email reports to configured recipients

### RAG Chatbot System
- **Intelligent Q&A**: Ask natural language questions about job postings
- **Vector Search**: ChromaDB-powered semantic search across job documents
- **LLM Integration**: Groq/Llama-3 powered responses with rich formatting
- **Auto-Ingestion**: Automatic processing of new documents into the knowledge base
- **Odoo Integration**: Direct integration with Odoo job posting data

### Frontend Dashboard
- **React-based UI**: Modern Vite + React frontend
- **Real-time Statistics**: Live processing statistics and system status
- **Job Management**: Browse, filter, and manage job postings
- **Chatbot Interface**: Interactive chatbot for querying job information

## ğŸ“‹ Prerequisites

- Python 3.8+
- Node.js 16+
- Google Gmail API credentials
- ChromeDriver (for web scraping)
- Odoo API access (optional)

## ğŸ”§ Installation

### Backend Setup

1. **Clone the repository**
   ```bash
   cd backend3
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   Create a `.env` file in the root directory:
   ```env
   # Add your environment variables here
   GROQ_API_KEY=your_groq_api_key
   ODOO_URL=your_odoo_url
   ODOO_DB=your_odoo_database
   ODOO_USERNAME=your_username
   ODOO_PASSWORD=your_password
   ```

4. **Set up Gmail API credentials**
   - Place `credentials1.json` and `client.json` in the root directory
   - Run the application once to generate `token.json` and `token1.json`

### Frontend Setup

1. **Navigate to frontend directory**
   ```bash
   cd frontend
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Start development server**
   ```bash
   npm run dev
   ```

## ğŸš€ Running the Application

### Start Backend Server

```bash
python app.py
```

The backend server will start on `http://localhost:5000` and automatically:
- Initialize the RAG chatbot system
- Start continuous monitoring after 5 seconds
- Begin processing emails from configured portals

### Start Frontend Development Server

```bash
cd frontend
npm run dev
```

The frontend will be available at `http://localhost:5173` (default Vite port)

## ğŸ“¡ API Endpoints

### System Endpoints
- `GET /` - System information and available endpoints
- `GET /api/status` - Get system status
- `POST /api/start` - Run all portals once
- `POST /api/monitor/start` - Start continuous monitoring
- `POST /api/monitor/stop` - Stop monitoring
- `POST /api/email/send` - Send due list email
- `GET /api/stats` - Get processing statistics
- `POST /api/run/portal/<name>` - Run specific portal

### RAG Chatbot Endpoints
- `GET /rag/stats` - RAG system statistics
- `POST /rag/query` - Ask questions about jobs
- `GET /rag/sample-queries` - Example questions
- `GET /rag/odoo-postings` - Odoo posting statistics

## ğŸ—ï¸ Project Structure

```
backend3/
â”œâ”€â”€ api/                          # API route blueprints
â”‚   â”œâ”€â”€ unified_routes.py         # Main unified API routes
â”‚   â”œâ”€â”€ odoo_routes.py           # Odoo-specific routes
â”‚   â””â”€â”€ ...
â”œâ”€â”€ services/                     # Core business logic
â”‚   â”œâ”€â”€ unified/                 # Unified monitoring services
â”‚   â”œâ”€â”€ dir/                     # HHSC portal services
â”‚   â”œâ”€â”€ vms/                     # VMS portal services
â”‚   â”œâ”€â”€ odoo/                    # Odoo integration services
â”‚   â”œâ”€â”€ dual_table/              # Dual table processing
â”‚   â””â”€â”€ email/                   # Email services
â”œâ”€â”€ rag/                         # RAG chatbot system
â”‚   â”œâ”€â”€ chatbot_api.py          # Chatbot API endpoints
â”‚   â”œâ”€â”€ chroma_manager.py       # ChromaDB vector store
â”‚   â”œâ”€â”€ llm_generator.py        # LLM response generation
â”‚   â”œâ”€â”€ query_engine.py         # Query processing
â”‚   â””â”€â”€ ingestion_service.py    # Document ingestion
â”œâ”€â”€ utils/                       # Utility modules
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ logger.py               # Logging utilities
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/                    # React frontend
â”‚   â”œâ”€â”€ src/                    # Source files
â”‚   â”œâ”€â”€ public/                 # Static assets
â”‚   â””â”€â”€ package.json            # Frontend dependencies
â”œâ”€â”€ entry_points/               # Entry point scripts
â”œâ”€â”€ app.py                      # Main Flask application
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ” Security Notes

The following files contain sensitive information and are gitignored:
- `.env` - Environment variables
- `credentials*.json` - Google API credentials
- `client.json` - OAuth client configuration
- `token*.json` - OAuth tokens
- `sqlite_db/` - Database files
- `*_documents/` - Downloaded documents
- `chromadb_data/` - Vector database

**Never commit these files to version control!**

## ğŸ“Š Data Storage

- **SQLite Database**: Job tracking and metadata (`sqlite_db/`)
- **ChromaDB**: Vector embeddings for RAG system (`chromadb_data/`)
- **Excel Files**: Dual table job tracking (`job_tracker_report.xlsx`)
- **JSON Files**: Email and file processing tracking

## ğŸ¤– RAG System

The RAG (Retrieval-Augmented Generation) system provides intelligent question-answering capabilities:

1. **Document Ingestion**: Automatically processes PDFs, DOCX, and TXT files
2. **Vector Storage**: Uses ChromaDB with sentence-transformers for embeddings
3. **Semantic Search**: Finds relevant job information based on natural language queries
4. **LLM Generation**: Generates formatted responses using Groq/Llama-3

### Example Queries
- "What jobs are available in Austin?"
- "Show me all software developer positions"
- "What are the requirements for job ID 12345?"

## ğŸ› ï¸ Development

### Running Tests
```bash
python test_chatbot.py
python test_rag_filters.py
```

### Building Frontend for Production
```bash
cd frontend
npm run build
```

## ğŸ“ Monitoring Configuration

The system monitors:
- **HHSC Portal**: Every 15 seconds for new emails
- **VMS Portal**: Every 15 seconds for new emails
- **Daily Email**: Sent to `jobdescriptions1@gmail.com`
- **Processing**: Dual table system using Excel tracking

## ğŸ”„ Automation Flow

1. System starts and initializes RAG chatbot
2. After 5 seconds, continuous monitoring begins
3. Every 15 seconds, checks for new emails in both portals
4. Processes new emails and extracts job information
5. Updates dual table tracking system
6. Ingests documents into RAG system
7. Sends daily email reports

## ğŸ“ Support

For issues or questions, check the logs in the console output. The system provides detailed logging for all operations.

## ğŸ“„ License

[Add your license information here]

## ğŸ‘¥ Contributors

[Add contributor information here]
